<!DOCTYPE html><html lang="en" class="post"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Riley Shaw | Taking The Average Tone</title><meta name="description" content="Riley Shaw | A designer and programmer"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="msapplication-TileColor" content="#44CCAA"><meta name="msapplication-TileImage" content="/favicon-144.png"><link rel="apple-touch-icon-precomposed" href="/favicon-152.png"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/main.aac3.css"><link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400|Droid+Serif:400,700,400italic|Raleway:800|Montserrat:700|Inconsolata:400,700" rel="dns-prefetch preconnect stylesheet" type="text/css"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48709534-1', 'rileyjshaw.com');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');</script></head><body><div id="drawer-nav-overlay"></div><header id="top-nav"><a href="/" id="drawer-nav-link" class="logo" data-no-instant>R</a><nav><ul></ul></nav></header><nav id="drawer-nav" role="navigation"><ul><li><a href="/" class="home-block"><div class="vcent"></div><span>Home</span></a></li><li><a href="/about" class="about-block"><div class="vcent"></div><span>About</span></a></li><li><a href="/blog" class="blog-block"><div class="vcent"></div><span>Blog</span></a></li><li><a href="/lab" class="lab-block"><div class="vcent"></div><span>Lab</span></a></li></ul></nav><div class="post-content"><div class="inner"><article><h1>Taking the average tone</h1><p class="date">Posted 5 years and 6 months ago</p><p>Today I wanted to see what the average frequency of a song would sound like with no spectrum analysis or separation. I had a hunch that it would end up sounding like garbage, and I was totally right.</p><p>If you took the average color of a beautiful painting, it would likely turn out poo brown. Today, I created the audio equivalent:</p><p><a href="https://youtu.be/rlyYQlPrdac">Blazo’s Misty Sapphire</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139984944&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p><a href="https://youtu.be/E_jwv2QMtAo">Steve Reich’s Music for 18 Musicians</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139984941&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p><a href="https://youtu.be/f-6H8NsfPNQ">Kill the Noise remix of KOAN Sound’s Talk Box</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139984943&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p>These first tones were generated using the powerful, free, cross-platform audio software <a href="http://audacity.sourceforge.net/">Audacity</a> and a cool lisp-y language called <a href="https://en.wikipedia.org/wiki/Nyquist_(programming_language)">Nyquist</a>. Since Audacity lets you run Nyquist scripts on hand-selected audio segments, this first bit was quick and dirty:</p><ol><li>Drag the desired audio file into <a href="http://audacity.sourceforge.net/">Audacity</a></li><li>On the left side of the window, click the arrow<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> beside the track’s name</li><li>Select “Split Stereo to Mono” from the resulting dropdown</li><li>Double-click the top track to select it</li><li>In the menu bar, click Effect &gt; Nyquist Prompt…</li><li>Enter the following into the prompt:<pre><code class="language-lisp">
   (setf f0 (aref (yin s 33 93 4400) 0))
   (setf fl (truncate (snd-length f0 ny:all)))
   (setf mean-f0 (snd-fetch (snd-avg f0 fl fl op-average)))
   (format nil "Mean Fundamental Frequency:~%~a ~~~a"
           (step-to-hz mean-f0)
           (nth (round mean-f0) nyq:pitch-names))
 </code></pre></li><li>If you’re using a recent version of Audacity, check the “Use legacy (version 3) syntax” box at the top of the prompt<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup></li><li>Let it run and <strong>remember the output frequency</strong></li><li>Keeping the track selected, click Generate &gt; Tone… in the top menu</li><li>Input the frequency that was generated in step 7</li><li>Repeat 4 - 9 on the bottom track</li><li>Click the arrow again and select “Make Stereo Track”</li></ol><p>It’s hack-y and too manual, but it was enough to show me that I definitely didn’t want to keep going down this path. From a spectrum this wide, across such a long duration, the tones I was getting didn’t correspond to any individual sound in the original song. At best, I’d found a roundabout way to generate spooky alien noises.</p><h2 id="measures-of-central-tendency">Measures of central tendency</h2><p>Let’s quickly go over <em>mean</em>, <em>median</em>, and <em>mode</em>.</p><ul><li><strong>Mean</strong>, often called <em>average</em>, is the sum of all values in a set divided by the number of values in the set</li><li><strong>Median</strong> is the middle-point in a set ordered by magnitude</li><li><strong>Mode</strong>, the ugly cousin of median, is the most commonly occurring value in a set</li></ul><p>Given the following set of numbers: { <strong>8, 4, 6, 6 ,8, 4, 8</strong> },</p><p>and rearranging them in order: { <strong>4, 4, 6, 6, 8, 8, 8</strong> },</p><p>we observe a <em>median</em> of 6, a <em>mode</em> of 8, and a <em>mean</em> of… 6.29?</p><h2 id="why-mean-sometimes-sucks">Why mean sometimes sucks</h2><p>Our result above demonstrates why my “average” tones sound nothing like their original tracks. After taking the mean of a set, there’s no guarantee that your result will be a member of the set. Just as 6.29 isn’t part of the set { <strong>8, 4, 6, 6 ,8, 4, 8</strong> }, the tones that I generated weren’t <em>really</em> part of their respective songs. In the process of finding the “average” tone, we lost touch with the most important part of the song; its individual notes.</p><h2 id="more-resolution">More resolution!</h2><p>After showing this to <a href="https://twitter.com/f06io">a friend of mine</a>, he suggested that I take the average frequency of each quarter-note instead of taking it across the entire song. Remember, this in <em>no way</em> addresses the problem that we just went over.</p><p>“It will still sound terrible,” I said, “the drums and bass and vocals are going to be mixed into some weird middle-ground tone”.</p><p>“Yeah,” he replied, “but it’ll be a different <em>kind</em> of terrible.”</p><p>I couldn’t argue with that. It would be near-impossible to use my old method on a per-note basis since that would require hand-selecting each quarter-note, so I decided to do a better job at it using Python.</p><p>Python has some awesome modules for audio analysis, but the underlying math can be pretty intimidating. Though a lot of the heavy-lifting is abstracted away for us, it’s important to have a grasp of a few <em>ideas</em> before diving in. Since we’re trying to find the average frequency of a tone, let’s start by figuring out how to isolate a frequency in a signal!</p><h2 id="finding-frequencies">Finding frequencies</h2><p>When we talk about frequencies in music, we’re referring to the <em>frequency</em> at which a <em>pressure wave needs to repeat to make our ears hear a certain note</em><sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. Audio signals are usually represented as a pressure wave over time since that’s how microphones and speakers understand sound. When multiple frequencies are played at the same time we end up with a crazy waveform that represents the composite of its individual parts. If we’re really clever, we might be able to pick out characteristic waveforms of different sounds just by how they look.</p><figure><img src="/images/hello_there_audio.9bca.png" width="250px" alt="Audio waveform for 'hello there'"><figcaption>Hello there</figcaption></figure><p>A time-domain representation like this allows us to recognize the structure of a song and to move sections around intuitively. One trade-off of representing audio like this is that filtering or modifying <em>frequency</em> content becomes really difficult as you add more sounds. To play with frequencies, we’re going to move from the time-domain into the <em>frequency-domain</em>.</p><h2 id="the-fourier-transform">The Fourier Transform</h2><p>The Fourier transform allows us to move from the time or spatial representations of signals that we’re used to into the frequency domain. Calculating the Fourier transform of an audio signal gives you a new representation that looks like this:</p><figure><img src="/images/c_major_freq.c6af.png" width="400px" alt="C Major chord in the frequency domain"><figcaption>C major chord in the frequency-domain</figcaption></figure><p>Here we have frequency on the x-axis and magnitude on the y-axis. The Fourier transform breaks a signal down into its component frequencies; since the diagram above shows high magnitudes at about <em>260Hz</em> (<em>C</em>), <em>330Hz</em> (<em>E</em>), and <em>390Hz</em> (<em>G</em>), it looks like we’ve got a <strong>C Major chord</strong> on our hands. Easier than trying to figure out the chord from the time-domain, huh?</p><figure><img src="/images/c_major_time.b758.png" width="400px" alt="C Major chord in the time domain"><figcaption>C major chord in the time-domain</figcaption></figure><p>Wikipedia has the best gif I’ve ever seen for this:</p><figure><img src="https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif" alt="Fourier transform between the time and frequency domains"><figcaption>Fourier transform between the time and frequency domains</figcaption></figure><h2 id="got-it">Got it.</h2><p>If you didn’t understand all of that, don’t worry. The important thing to remember is that we need to figure out how powerful each frequency is, and the Fourier transform helps us get there.</p><p>First we import a few modules<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>:</p><pre><code class="language-python">import numpy as np, composer
import scipy.io.wavfile as wav
import scipy.fftpack as fft
from itertools import izip_longest
</code></pre><p>Next, we use scipy’s <code class="language-python">wavfile.read</code> to return the audio’s sample rate and audio contents.</p><pre><code class="language-python">import_rate, import_data = wav.read('wav/flute.wav')
import_bpm = 62 #manually set
</code></pre><p>…and we’re all set up. Finding the average frequency of an audio clip is going to be the meatiest part of this problem, and it’s not too bad with the help of scipy and numpy:</p><pre><code class="language-python">def average_frequency(rate, data):
    sample_length = len(data)
    k = np.arange(sample_length)
    period = sample_length / rate
    freqs = (k / period)[range(sample_length / 2)] #right-side frequency range
    fourier = abs(fft.fft(data * np.hanning(sample_length)) / sample_length) #normalized, not clipped
    fourier = fourier[range(sample_length / 2)] #clip to right-side
    power = np.power(fourier, 2.0)
    return sum(power * freqs) / sum(power)
</code></pre><p>Essentially, we’re being passed in some audio content and its corresponding sample rate, throwing it through a Fourier transform, and returning the average frequency <em>as heard by us</em>. The magnitudes alone don’t mean much physically, so we square them to represent the <em>power</em> of each frequency before averaging the range.</p><p>Next we need to split the song into chunks to send through the <code class="language-python">average_frequency</code> function. Since we know our sample rate (samples / second) and tempo (beats / minute) already, we can figure out how many samples make up a beat and slice it up.</p><pre><code class="language-python">def quarter_note_frequencies(rate, data, bpm):
    notes = []
    beat_counter = 0
    slice_size = rate * 60 / bpm #samples per beat
    beats = len(data) / slice_size #beats per song
    for slice in grouper(data, slice_size, 0):
        beat_counter += 1
        print unicode(beat_counter * 100 / beats) + '% completed'
        notes.append(average_frequency(rate * 1.0, slice))
    return notes
</code></pre><p>Here we’re using a function called <code class="language-python">grouper</code> that lets us slice up the data. It’s described in depth in <a href="https://docs.python.org/2/library/itertools.html#recipes">itertools’ Recipes section</a>.</p><p>We’re getting really close! Now we just need a function to write some frequencies to a wav file,</p><pre><code class="language-python">def create_wav(rate, data, bpm):
    duration = 60.0 / bpm #seconds per beat
    wav.write('wav/flute_avg.wav', rate, np.array(composer.generate_tone_series(data, duration), dtype = np.int16))
</code></pre><p>…and a line to call that file…</p><pre><code class="language-python">create_wav(import_rate, quarter_note_frequencies(import_rate, import_data, import_bpm), import_bpm)
</code></pre><p>And we’re done! The whole thing (including my <code class="language-python">composer</code> module), is on <a href="https://github.com/rileyjshaw/mean-tone">Github</a>.</p><p>Let’s hear what it sounds like:</p><p><a href="https://youtu.be/LFQhbZ7Kkig">Them Crooked Vultures’ Bandoliers</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139987561&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p><a href="https://youtu.be/oMu9H7Bkb1Y">The Chemical Brothers’ Another World</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139984938&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p><a href="https://youtu.be/AQLh3WanSfg">Brian Eno’s 2-1</a>:</p><iframe width="100%" height="124" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/139985315&amp;color=ff5500&amp;auto_play=false&amp;hide_related=true&amp;show_artwork=false"></iframe><p>Wow! Still terrible! Who would’ve thought!</p><h2 id="conclusion">Conclusion</h2><p>It’s probably safe to say that this isn’t going to be the next-big-thing in music. That said, it was really cool that some songs (see “Another World” above) output a repeating melody different from their own. It’s entirely predictable, but I still enjoyed being able to hear it.</p><p>I hope to make a Part Two some day where I explore some nicer-sounding alternatives, but I’m hanging up my signal processing hat for a while to focus on other things. One very fun project would be to take a “moving window” through a song and record only the highest magnitude frequencies at each time-step. Doing this, you could subvert the skewing caused by loud, quick tones (snare drum) and artifacts caused by strange envelopes or effects. It isn’t a big step from where we got to today, and I’m confident that with enough tuning it could roughly isolate a song’s melody.</p><h2 id="footnotes">Footnotes</h2><div class="footnotes"><ol><li id="fn:1"><p>The arrow looks like this:</p><figure><img src="/images/audacity_arrow.4ab1.png" width="163px" alt="Audacity track side-menu"></figure><p><a href="#fnref:1" class="reversefootnote">&#8617;</a></p></li><li id="fn:2"><p>Thanks Nolan Beck for the heads up!</p><figure><img src="/images/audacity_legacy_checkbox.903d.png" width="406px" alt="Nyquist legacy syntax option"></figure><p><a href="#fnref:2" class="reversefootnote">&#8617;</a></p></li><li id="fn:3"><p>A 440Hz sine wave, for example, sounds like an <strong>A</strong> to our ears. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p></li><li id="fn:4"><p>Including one I wrote called <code class="language-python">composer</code>, which generates an audio signal from the frequencies we pass it <a href="#fnref:4" class="reversefootnote">&#8617;</a></p></li></ol></div></article><div id="disqus_thread"></div><script>var disqus_shortname = 'rileyjshaw';
          (function() {
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();</script><a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div id="discussion"></div></div></div><footer><strong>Share this:</strong><ul><li><a href="https://twitter.com/home?status=I%20just%20read%20%22Taking the average tone%22%20on%20http://rileyjshaw.com/blog/taking-the-average-tone%20and%20it%20legitimately%20changed%20my%20life."><i class="fa fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://rileyjshaw.com//blog/taking-the-average-tone"><i class="fa fa-facebook"></i></a></li></ul></footer><script src="/scripts/script.6400.js"></script></body></html>